{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m heatmap_j_max \u001b[38;5;241m=\u001b[39m heatmap_j\u001b[38;5;241m.\u001b[39mmax(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     72\u001b[0m heatmap_j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m heatmap_j_max\n\u001b[0;32m---> 73\u001b[0m heatmap_j \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheatmap_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreserve_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m cmap \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mget_cmap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     76\u001b[0m heatmap_j2 \u001b[38;5;241m=\u001b[39m cmap(heatmap_j,alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/skimage/transform/_warps.py:162\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    160\u001b[0m order \u001b[38;5;241m=\u001b[39m _validate_interpolation_order(input_type, order)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m order \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Save input value range for clip\u001b[39;00m\n\u001b[1;32m    165\u001b[0m img_bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([image\u001b[38;5;241m.\u001b[39mmin(), image\u001b[38;5;241m.\u001b[39mmax()]) \u001b[38;5;28;01mif\u001b[39;00m clip \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/skimage/_shared/utils.py:687\u001b[0m, in \u001b[0;36mconvert_to_float\u001b[0;34m(image, preserve_range)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_range:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Convert image to double only if it is not single or double\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# precision float\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    688\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'char'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "\n",
    "\n",
    "class GradCamModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gradients = None\n",
    "        self.tensorhook = []\n",
    "        self.layerhook = []\n",
    "        self.selected_out = None\n",
    "        \n",
    "        #PRETRAINED MODEL\n",
    "        self.pretrained = models.resnet50(pretrained=True)\n",
    "        #self.pretrained = models.alexnet(pretrained=True)\n",
    "        #self.pretrained = models.mobilenet_v2(pretrained=True)\n",
    "        \n",
    "\n",
    "        self.layerhook.append(self.pretrained.layer3.register_forward_hook(self.forward_hook()))\n",
    "\n",
    "        for name, param in self.pretrained.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def activations_hook(self,grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def get_act_grads(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def forward_hook(self):\n",
    "        def hook(module, inp, out):\n",
    "            self.selected_out = out\n",
    "            self.tensorhook.append(out.register_hook(self.activations_hook))\n",
    "        return hook\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.pretrained(x)\n",
    "        return out, self.selected_out\n",
    "\n",
    "\n",
    "gcmodel = GradCamModel()#.to('cuda:0')\n",
    "img = imread('tiger.jfif') #'bulbul.jpg'\n",
    "img = resize(img, (224,224), preserve_range = True)\n",
    "img = np.expand_dims(img.transpose((2,0,1)),0)\n",
    "img /= 255.0\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape((1,3,1,1))\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape((1,3,1,1))\n",
    "img = (img - mean)/std\n",
    "inpimg = torch.from_numpy(img).to(torch.float32)\n",
    "\n",
    "out, acts = gcmodel(inpimg)\n",
    "acts = acts.detach().cpu()\n",
    "\n",
    "loss = nn.CrossEntropyLoss()(out,torch.from_numpy(np.array([600])))#.to('cuda:0'))\n",
    "loss.backward()\n",
    "\n",
    "grads = gcmodel.get_act_grads().detach().cpu()\n",
    "pooled_grads = torch.mean(grads, dim=[0,2,3]).detach().cpu()\n",
    "\n",
    "for i in range(acts.shape[1]):\n",
    "    acts[:,i,:,:] += pooled_grads[i]\n",
    "\n",
    "heatmap_j = torch.mean(acts, dim = 1).squeeze()\n",
    "heatmap_j_max = heatmap_j.max(axis = 0)[0]\n",
    "heatmap_j /= heatmap_j_max\n",
    "heatmap_j = resize(heatmap_j,(224,224),preserve_range=True)\n",
    "\n",
    "cmap = mpl.cm.get_cmap('jet', 256)\n",
    "heatmap_j2 = cmap(heatmap_j,alpha = 0.2)\n",
    "fig, axs = plt.subplots(1,1,figsize = (5,5))\n",
    "axs.imshow((img*std+mean)[0].transpose(1,2,0))\n",
    "axs.imshow(heatmap_j)\n",
    "plt.show()\n",
    "\n",
    "for h in gcmodel.layerhook:\n",
    "    h.remove()\n",
    "for h in gcmodel.tensorhook:\n",
    "    h.remove()\n",
    "    \n",
    "    #Alexnet vgg16 - features\n",
    "    #resnet50 - layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "m,n = img.shape[::2]\n",
    "print(m)\n",
    "print(n)\n",
    "data_new = img.reshape(n,n,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-oe0iat4a/opencv/modules/core/src/arithm.cpp:669: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-094b8ffdfe9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWeighted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-oe0iat4a/opencv/modules/core/src/arithm.cpp:669: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n"
     ]
    }
   ],
   "source": [
    "fi = cv2.addWeighted(np.float32(heatmap_j), 0.7, img, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'index' is required to be an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-41e394d2b07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcapture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'index' is required to be an integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cv2 import cv2\n",
    "from skimage import exposure \n",
    "...\n",
    "\n",
    "capture = cv2.VideoCapture(...)\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if ret:\n",
    "        #resize original frame\n",
    "        frame = cv2.resize(frame, (224, 224)) \n",
    "\n",
    "        #get color map\n",
    "        cam = getMap(frame)\n",
    "        map_img = exposure.rescale_intensity(cam, out_range=(0, 255))\n",
    "        map_img = np.uint8(map_img)\n",
    "        heatmap_img = cv2.applyColorMap(map_img, cv2.COLORMAP_JET)\n",
    "\n",
    "        #merge map and frame\n",
    "        fin = cv2.addWeighted(heatmap_img, 0.5, frame, 0.5, 0)\n",
    "\n",
    "        #show result\n",
    "        cv2.imshow('frame', fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/Users/nihalmathew/Downloads/NEU-CA-main/Spring2022/DV/Project/image_classification/tiger.jfif', 1)\n",
    "gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_img = cv2.applyColorMap(gray_img, cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 275)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
